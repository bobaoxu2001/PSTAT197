summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
test_summaries <- test_accuracy %>%
rename(roc.auc = roc_auc) %>%
select(-partition) %>%
summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
left_join(train_summaries,
test_summaries,
by = 'metric',
suffix = c('.train', '.test')) %>%
select(metric, contains('mean'), contains('sd')) %>%
knitr::kable()
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R'
source(url)
# load packages
library(tidyverse)
library(tidymodels)
library(modelr)
library(rsample)
library(yardstick)
# read data
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab4-logistic/data/biomarker_clean.csv'
s_star <- c("DERM", "RELT", "IgD", "PTN", "FSTL1")
biomarker <- read_csv(url) %>%
# subset to proteins of interest and group
select(group, any_of(s_star)) %>%
# convert group (chr) to binary (lgl)
mutate(class = (group == 'ASD')) %>%
select(-group)
# for reproducibility
set.seed(102022)
# partition data
partitions <- biomarker %>%
initial_split(prop = 0.8)
# examine
partitions
# for reproducibility
set.seed(102022)
# partition data
partitions <- biomarker %>%
initial_split(prop = 0.8)
# examine
partitions
# training set
training(partitions) %>% head(4)
# training set
training(partitions) %>% head(4)
# testing set
testing(partitions) %>% head(4)
# fit glm
fit <- glm(class ~ .,
data = biomarker,
family = binomial(link = "logit"))
tidy(fit)
# compute predictions on the test set
testing(partitions) %>%
add_predictions(fit)
# manually transform to probabilities
testing(partitions) %>%
add_predictions(fit) %>%
mutate(probs = 1/(1 + exp(-pred))) %>%
select(class, pred, probs) %>%
head(5)
# predict on scale of response
testing(partitions) %>%
add_predictions(fit, type = 'response') %>%
select(class, pred) %>%
head(5)
# predict classes
testing(partitions) %>%
add_predictions(fit, type = 'response') %>%
mutate(pred.class = (pred > 0.5)) %>%
select(class, pred, pred.class) %>%
head(5)
# tabulate
testing(partitions) %>%
add_predictions(fit, type = 'response') %>%
mutate(pred.class = (pred > 0.5)) %>%
select(class, pred.class) %>%
table()
# store predictions as factors
pred_df <- testing(partitions) %>%
add_predictions(fit, type = 'response') %>%
mutate(pred.class = (pred > 0.5),
group = factor(class, labels = c('TD', 'ASD')),
pred.group = factor(pred.class, labels = c('TD', 'ASD')))
# check order of factor levels
pred_df %>% pull(group) %>% levels()
# compute specificity
pred_df %>%
specificity(truth = group,
estimate = pred.group,
event_level = 'second')
# compute specificity
pred_df %>%
specificity(truth = group,
estimate = pred.group,
event_level = 'second')
# sensitivity
pred_df %>%
sensitivity(truth = group,
estimate = pred.group,
event_level = 'second')
# accuracy
pred_df %>%
accuracy(truth = group,
estimate = pred.group,
event_level = 'second')
# define panel (arguments must be yardstick metric function names)
panel_fn <- metric_set(sensitivity, specificity)
# compute
pred_df %>%
panel_fn(truth = group,
estimate = pred.group,
event_level = 'second')
pred_df %>% conf_mat(truth = group, estimate = pred.group)
pred_df %>%
roc_curve(truth = group, estimate = pred)
pred_df %>%
roc_curve(truth = group,
estimate = pred,
event_level = 'second') %>%
ggplot(aes(y = sensitivity, x = 1 - specificity)) +
geom_path() +
geom_abline(slope = 1, intercept = 0)
pred_df %>% roc_auc(truth = group,
estimate = pred,
event_level = 'second')
# define some helper functions
fit_fn <- function(.df){
glm(class ~ ., family = 'binomial', data = .df)
}
pred_fn <- function(.df, .mdl){
.df %>% add_predictions(.mdl, type = 'response')
}
panel <- metric_set(sensitivity, specificity, accuracy, roc_auc)
eval_fn <- function(.df){
.df %>%
mutate(group = factor(class, labels = c('TD', 'ASD')),
pred.group = factor(pred > 0.5, labels = c('TD', 'ASD'))) %>%
panel(truth = group,
estimate = pred.group,
pred,
event_level = 'second')
}
set.seed(101922)
n_splits <- 400
out <- tibble(partition = 1:n_splits,
split = map(partition, ~ initial_split(biomarker, prop = 0.8)),
train = map(split, training),
test = map(split, testing),
fit = map(train, fit_fn),
pred_test = map2(test, fit, pred_fn),
pred_train = map2(train, fit, pred_fn),
eval_test = map(pred_test, eval_fn),
eval_train = map(pred_train, eval_fn))
out %>% head(4)
train_accuracy %>% head(4)
train_summaries <- train_accuracy %>%
rename(roc.auc = roc_auc) %>%
select(-partition) %>%
summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
test_summaries <- test_accuracy %>%
rename(roc.auc = roc_auc) %>%
select(-partition) %>%
summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
train_accuracy %>% head(4)
train_summaries <- train_accuracy %>%
rename(roc.auc = roc_auc) %>%
select(-partition) %>%
summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
test_summaries <- test_accuracy %>%
rename(roc.auc = roc_auc) %>%
select(-partition) %>%
summarize_all(.funs = list(mean = mean, sd = sd)) %>%
gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
left_join(train_summaries,
test_summaries,
by = 'metric',
suffix = c('.train', '.test')) %>%
select(metric, contains('mean'), contains('sd')) %>%
knitr::kable()
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R'
source(url)
# setup
library(tidyverse)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab5-text/data/drseuss.txt'
# read data
seuss_lines <- read_lines(url, skip_empty_rows = T)
seuss_lines %>% head()
# flag lines with a document id
seuss_lines_df <- tibble(line_lag = c(seuss_lines, NA)) %>%
mutate(flag = str_detect(line_lag, 'Dr. Seuss'),
line = lag(line_lag, n = 1),
doc = cumsum(flag)) %>%
select(doc, line) %>%
slice(-1) %>%
fill(doc)
# grab titles
titles <- seuss_lines_df %>%
group_by(doc) %>%
slice_head() %>%
pull(line) %>%
tolower()
# label docs
seuss_lines_df <- seuss_lines_df %>%
mutate(doc = factor(doc, labels = titles))
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
seuss_lines_clean
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
str_detect(seuss_lines_clean)
# collapse lines into one long string
seuss_text <- seuss_lines_clean %>%
summarize(text = str_c(line, collapse = ' '))
# collapse lines into one long string
seuss_text <- seuss_lines_clean %>%
summarize(text = str_c(line, collapse = ' '))
cat_in_hat <- seuss_text %>% slice(1) %>% pull(text)
cat_in_hat %>%
str_remove_all('[[:punct:]]') %>%
tolower()
clean_fn <- function(.text){
str_remove_all(.text, '[[:punct:]]') %>% tolower()
}
seuss_text_clean <- seuss_text %>%
mutate(text = clean_fn(text))
stpwrd <- stop_words %>%
pull(word) %>%
str_remove_all('[[:punct:]]')
seuss_tokens_long <- seuss_text_clean %>%
unnest_tokens(output = token, # specifies new column name
input = text, # specifies column containing text
token = 'words', # how to tokenize
stopwords = stpwrd) %>% # optional stopword removal
mutate(token = lemmatize_words(token))
seuss_tfidf <- seuss_tokens_long %>%
count(doc, token) %>%
bind_tf_idf(term = token,
document = doc,
n = n)
seuss_df <- seuss_tfidf %>%
pivot_wider(id_cols = doc,
names_from = token,
values_from = tf_idf,
values_fill = 0)
seuss_df
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf_idf, n = 2)
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf, n = 2)
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf, n = 2)
knitr::opts_chunk$set(echo = TRUE)
library(keras)
install_keras()
Y
knitr::opts_chunk$set(echo = TRUE)
install.packages('keras')
library(keras)
install_keras()
knitr::opts_chunk$set(echo = TRUE)
library(tensorflow)
tf$constant('Hello world')
#install.packages('keras')
library(keras)
#install_keras()
library(tensorflow)
tf$constant('Hello world')
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 50)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
plot(history)
# redefine model
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# train with validation split
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 20,
validation_split = 0.2)
plot(history)
knitr::opts_chunk$set(echo = TRUE)
#install.packages('keras')
library(keras)
#install_keras()
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# retrieve weights
# get_weights(model)
load("~/Documents/2022Fall/Pstat197/claims-group-12/results/preds-group[N].RData")
source('https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R')
# packages
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
install.packages("Matrix")
install.packages("Matrix")
load("/Users/xuao/Downloads/f22-claims-evals.RData")
install.packages("Matrix")
install.packages("Matrix")
