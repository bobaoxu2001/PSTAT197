gather() %>%
separate(key, into = c('metric', 'stat'), sep = '_') %>%
spread(stat, value)
left_join(train_summaries,
test_summaries,
by = 'metric',
suffix = c('.train', '.test')) %>%
select(metric, contains('mean'), contains('sd')) %>%
knitr::kable()
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R'
source(url)
# setup
library(tidyverse)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab5-text/data/drseuss.txt'
# read data
seuss_lines <- read_lines(url, skip_empty_rows = T)
seuss_lines %>% head()
# flag lines with a document id
seuss_lines_df <- tibble(line_lag = c(seuss_lines, NA)) %>%
mutate(flag = str_detect(line_lag, 'Dr. Seuss'),
line = lag(line_lag, n = 1),
doc = cumsum(flag)) %>%
select(doc, line) %>%
slice(-1) %>%
fill(doc)
# grab titles
titles <- seuss_lines_df %>%
group_by(doc) %>%
slice_head() %>%
pull(line) %>%
tolower()
# label docs
seuss_lines_df <- seuss_lines_df %>%
mutate(doc = factor(doc, labels = titles))
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
seuss_lines_clean
# remove header lines (title/author)
seuss_lines_clean <- seuss_lines_df %>%
group_by(doc) %>%
mutate(line_num = row_number() - 2) %>%
filter(line_num > 0)
str_detect(seuss_lines_clean)
# collapse lines into one long string
seuss_text <- seuss_lines_clean %>%
summarize(text = str_c(line, collapse = ' '))
# collapse lines into one long string
seuss_text <- seuss_lines_clean %>%
summarize(text = str_c(line, collapse = ' '))
cat_in_hat <- seuss_text %>% slice(1) %>% pull(text)
cat_in_hat %>%
str_remove_all('[[:punct:]]') %>%
tolower()
clean_fn <- function(.text){
str_remove_all(.text, '[[:punct:]]') %>% tolower()
}
seuss_text_clean <- seuss_text %>%
mutate(text = clean_fn(text))
stpwrd <- stop_words %>%
pull(word) %>%
str_remove_all('[[:punct:]]')
seuss_tokens_long <- seuss_text_clean %>%
unnest_tokens(output = token, # specifies new column name
input = text, # specifies column containing text
token = 'words', # how to tokenize
stopwords = stpwrd) %>% # optional stopword removal
mutate(token = lemmatize_words(token))
seuss_tfidf <- seuss_tokens_long %>%
count(doc, token) %>%
bind_tf_idf(term = token,
document = doc,
n = n)
seuss_df <- seuss_tfidf %>%
pivot_wider(id_cols = doc,
names_from = token,
values_from = tf_idf,
values_fill = 0)
seuss_df
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf_idf, n = 2)
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf, n = 2)
seuss_tfidf %>%
group_by(doc) %>%
slice_max(tf, n = 2)
knitr::opts_chunk$set(echo = TRUE)
library(keras)
install_keras()
Y
knitr::opts_chunk$set(echo = TRUE)
install.packages('keras')
library(keras)
install_keras()
knitr::opts_chunk$set(echo = TRUE)
library(tensorflow)
tf$constant('Hello world')
#install.packages('keras')
library(keras)
#install_keras()
library(tensorflow)
tf$constant('Hello world')
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# evaluate on specified data
evaluate(model, x_train, y_train)
# compute predictions
model(x_train) %>% head()
# store full DTM as a matrix
x_train <- train_dtm %>%
select(-bclass, -.id) %>%
as.matrix()
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
summary(model)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 50)
plot(history)
# change the optimizer
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# re-train
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
plot(history)
# redefine model
model <- keras_model_sequential(input_shape = ncol(x_train)) %>%
layer_dense(10) %>%
layer_dense(1) %>%
layer_activation(activation = 'sigmoid')
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = 'binary_accuracy'
)
# train with validation split
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 20,
validation_split = 0.2)
plot(history)
knitr::opts_chunk$set(echo = TRUE)
#install.packages('keras')
library(keras)
#install_keras()
library(tensorflow)
tf$constant('Hello world')
# packages
library(tidyverse)
library(tidymodels)
library(tidytext)
library(keras)
library(tensorflow)
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab6-nn/data/claims-clean.csv'
# read in data
clean <- read_csv(url)
# partition
set.seed(102722)
partitions <- clean %>%
mutate(text_clean = str_trim(text_clean)) %>%
filter(str_length(text_clean) > 5) %>%
initial_split(prop = 0.8)
train_dtm <- training(partitions) %>%
unnest_tokens(output = 'token',
input = text_clean) %>%
group_by(.id, bclass) %>%
count(token) %>%
bind_tf_idf(term = token,
document = .id,
n = n) %>%
pivot_wider(id_cols = c(.id, bclass),
names_from = token,
values_from = tf_idf,
values_fill = 0) %>%
ungroup()
# extract first ten features
x_train <- train_dtm %>%
ungroup() %>%
select(-.id, -bclass) %>%
select(1:10) %>%
as.matrix()
# extract labels and coerce to binary
y_train <- train_dtm %>%
pull(bclass) %>%
factor() %>%
as.numeric() - 1
# specify model type
model <- keras_model_sequential(input_shape = 10)
summary(model)
# add output layer
model <- model %>% layer_dense(1)
summary(model)
model <- model %>%
layer_activation(activation = 'sigmoid')
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_sgd(),
metrics = 'binary_accuracy'
)
history <- model %>%
fit(x = x_train,
y = y_train,
epochs = 10)
# retrieve weights
get_weights(model)
# retrieve weights
# get_weights(model)
load("~/Documents/2022Fall/Pstat197/claims-group-12/results/preds-group[N].RData")
source('https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/scripts/package-installs.R')
# packages
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
install.packages("Matrix")
install.packages("Matrix")
load("/Users/xuao/Downloads/f22-claims-evals.RData")
install.packages("Matrix")
install.packages("Matrix")
library(tidyverse)
library(lubridate)
library(modelr)
library(fda)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab7-curvefitting/data/soiltemp.csv'
soil <- read_csv(url)
soil %>% head()
# scatterplot of temperatures against day of year
temp_pts <- soil %>%
ggplot(aes(x = day, y = temp)) +
geom_point(alpha = 0.1)
temp_pts
temp_pts +
geom_smooth(formula = 'y ~ x',
method = 'loess',
span = 0.5,
se = F)
temp_pts +
geom_smooth(formula = 'y ~ x',
method = 'loess',
span = 1,
se = F)
# quadratic fit
temp_pts +
geom_smooth(formula = 'y ~ poly(x, 2)',
method = 'lm')
poly(1:5, degree = 3, raw = T, simple = T)
# fit a polynomial model
fit_poly <- lm(temp ~ poly(day, degree = 2, raw = T),
data = soil)
# compute predictions
pred_df <- tibble(day = 1:365) %>%
add_predictions(fit_poly)
# visualize
temp_pts +
geom_path(data = pred_df,
aes(y = pred),
color = 'blue')
# linear spline with a knot at day 200
fit_spline <- lm(temp ~ day + I((day - 200)*(day > 200)) - 1,
data = soil)
# compute predictions
pred_df <- tibble(day = 1:365) %>%
add_predictions(fit_spline)
# plot it
temp_pts +
geom_path(data = pred_df,
aes(y = pred),
color = 'blue')
# define knot points
knotpts <- c(100, 200, 300)
# fit an order 3 regression spline with three internal knots
fit_bs <- lm(temp ~ bs(day, degree = 1, knots = knotpts),
data = soil)
# compute predictions
pred_df <- tibble(day = 1:365) %>%
add_predictions(fit_bs)
# plot it
temp_pts +
geom_path(data = pred_df,
aes(y = pred),
color = 'blue') +
geom_vline(xintercept = knotpts,
linetype = 'dashed')
# define knots
knotpts <- c(100, 200, 300)
# input variable
x <- 1:365
# calculate basis expansion and plot it
bs(x, knots = knotpts, degree = 3) %>%
as_tibble() %>%
bind_cols(x = x) %>%
pivot_longer(-x, names_to = 'basis') %>%
ggplot(aes(x = x, y = value)) +
geom_path(aes(group = basis, color = basis)) +
geom_vline(xintercept = knotpts, linetype = 'dashed')
# fit the model with the fourier basis expansion
fit_fbs <- lm(temp ~ fourier(day, nbasis = 4, period = 365) - 1,
data = soil)
# compute predictions
pred_df <- tibble(day = 1:365) %>%
add_predictions(fit_fbs)
# plot it
temp_pts +
geom_path(data = pred_df,
aes(y = pred),
color = 'blue')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(forecast)
library(fda)
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab8-forecasting/data/soiltemp-200cm.csv'
soil <- read_csv(url) %>%
dplyr::select(-year, -elev) %>%
filter(!str_starts(site, 'SHA'))
# choose a site at random
set.seed(111522) # comment out!
nsites <- soil %>% pull(site) %>% unique() %>% length()
site_ix <- sample(1:nsites, size = 1)
# filter rows
site_data <- soil %>%
filter(site == unique(soil$site)[site_ix])
# preview
site_data %>% head()
# predictor matrix
xreg <- site_data %>%
pull(day) %>%
fda::fourier(nbasis = 4, period = 365)
# response
y <- pull(site_data, temp)
# create a data frame
reg_df <- bind_cols(temp = y,
xreg)
# fit the model
fit <- lm(temp ~ . - 1, data = reg_df)
# obtain fitted values, residuals, etc.
fit_df <- broom::augment(fit) %>%
bind_cols(date = site_data$date)
# plot residual series
fit_df %>%
ggplot(aes(x = date, y = .resid)) +
geom_path()
# plot residuals at various lags
fit_df %>%
dplyr::select(.resid) %>%
mutate(lag1 = lag(.resid, n = 1),
lag2 = lag(.resid, n = 2),
lag3 = lag(.resid, n = 3),
lag4 = lag(.resid, n = 4),
lag5 = lag(.resid, n = 5),
lag6 = lag(.resid, n = 6)) %>%
pivot_longer(-.resid) %>%
ggplot(aes(x = .resid, y = value)) +
geom_point() +
facet_wrap(~ name)
resid_acf <- acf(fit_df$.resid, plot = F)
plot(resid_acf, main = '')
resid_pacf <- pacf(fit_df$.resid, plot = F)
plot(resid_pacf, main = '')
# fit error model
fit_resid <- Arima(fit_df$.resid,
order = c(2, 0, 0),
include.mean = F,
method = 'ML')
resid_acf_fitted <- ARMAacf(ar = coef(fit_resid),
lag.max = 25)
plot(resid_acf, main = '')
lines(resid_acf_fitted, col = 'red')
predict(fit_resid, n.ahead = 5)
forecast(fit_resid, h = 5)
# determine a point at which to cut the series
cutpt <- nrow(xreg) - 30
# training series
y_train <- y[1:cutpt]
x_train <- xreg[1:cutpt, ]
# fit the model
fit_full <- Arima(y_train,
order = c(2, 0, 0),
xreg = x_train,
include.mean = F,
method = 'ML')
broom::tidy(fit_full) %>% knitr::kable()
# testing series
y_test <- y[(cutpt + 1):nrow(xreg)]
x_test <- xreg[(cutpt + 1):nrow(xreg), ]
preds <- forecast(fit_full,
h = nrow(x_test),
xreg = x_test)
preds %>% as_tibble() %>% head()
fig_forecast <- site_data %>%
dplyr::select(date, temp) %>%
bind_cols(pred = c(fit_full$fitted, preds$mean),
status = c(rep('obs', nrow(x_train)),
rep('pred', nrow(x_test)))) %>%
filter(date >= ymd('2018-01-01')) %>% # adjust if needed
ggplot(aes(x = date)) +
geom_path(aes(y = temp, linetype = status)) +
geom_path(aes(y = pred), color = 'blue', alpha = 0.5)
fig_forecast
ci_df <- site_data %>%
slice_tail(n = nrow(x_test)) %>%
dplyr::select(date) %>%
bind_cols(lwr = preds$lower[, 2],
upr = preds$upper[, 2])
fig_forecast +
geom_ribbon(aes(x = date, ymin = lwr, ymax = upr),
alpha = 0.3, fill = 'blue',
data = ci_df)
