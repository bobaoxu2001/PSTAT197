s <- sd(college_edu)
n
s
t <- (x_bar - 0.32)/(s/sqrt(n))
t
college_edu <- midwest$percollege
x_bar <- mean(college_edu)
n <- nrow(midwest) #sample_size
s <- sd(college_edu)
n
s
t <- (x_bar - 0.32)/(s/sqrt(n))
t
critical_value <- qt(p=0.05,df=(n-1))
college_edu <- midwest$percollege
x_bar <- mean(college_edu)
n <- nrow(midwest) #sample_size
s <- sd(college_edu)
n
s
t <- (x_bar - 0.32)/(s/sqrt(n))
t
critical_value <- qt(p=0.05,df=(n-1))
critical_value
t.test(college_edu,mu=32,alternative='less')
college_edu <- midwest$percollege
x_bar <- mean(college_edu)
n <- nrow(midwest) #sample_size
s <- sd(college_edu)
n
s
t <- (x_bar - 32)/(s/sqrt(n))
t
critical_value <- qt(p=0.05,df=(n-1))
critical_value
t.test(college_edu,mu=32,alternative='less')
df2 <- midwest %>%
filter(state=="OH" | state=="MI")
df2 %>%
group_by(state) %>%
summarize(mean(percollege))
df2 <- midwest %>%
filter(state=="OH" | state=="MI")
df2 %>%
group_by(state) %>%
summarize(mean(percollege))
n <- nrow(OH)
data(sleep)
sleep
ggplot(sleep, aes(group, extra)) + geom_boxplot()
t.test(extra ~ group, data = sleep, paired = TRUE)
data(sleep)
sleep
ggplot(sleep, aes(group, extra)) + geom_boxplot()
t.test(extra ~ group, data = sleep, paired = TRUE)
data(sleep)
sleep
df2 <- midwest %>%
filter(state=="OH" | state=="MI")
df2 %>%
group_by(state) %>%
summarize(mean(percollege))
summary(df %>% filter(state == "OH") %>% .$percollege)
summary(df2 %>% filter(state == "OH") %>% .$percollege)
data(sleep)
sleep
ggplot(sleep, aes(group, extra)) + geom_boxplot()
t.test(extra ~ group, data = sleep, paired = TRUE)
library(tidyverse)
# install.packages('infer') # execute once then comment out
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'
# function for outlier trimming
trim_fn <- function(x){
x[x > 3] <- 3
x[x < -3] <- -3
return(x)
}
# read in and preprocess data
asd <- read_csv(url) %>%
select(-ados) %>%
# log transform
mutate(across(.cols = -group, log10)) %>%
# center and scale
mutate(across(.cols = -group, ~ scale(.x)[, 1])) %>%
# trim outliers
mutate(across(.cols = -group, trim_fn))
library(tidyverse)
# install.packages('infer') # execute once then comment out
# data location
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'
# function for outlier trimming
trim_fn <- function(x){
x[x > 3] <- 3
x[x < -3] <- -3
return(x)
}
# read in and preprocess data
asd <- read_csv(url) %>%
select(-ados) %>%
# log transform
mutate(across(.cols = -group, log10)) %>%
# center and scale
mutate(across(.cols = -group, ~ scale(.x)[, 1])) %>%
# trim outliers
mutate(across(.cols = -group, trim_fn))
for(i in 1:4){
print(2*i)
}
flag_vals <- c(1, 2, 3, 4)
for(i in flag_vals){
out <- 2*i
print(out)
}
rslt <- rep(NA, 4)
for(i in 1:4){
rslt[i] <- 2*i
}
rslt
rslt <- rep(NA, 4)
input_vals <- c(15, 27, 3, 12.6)
for(i in 1:4){
rslt[i] <- 2*input_vals[i]
}
rslt
rslt <- rep(NA, 4)
input_vals <- rnorm(n = 3)
for(i in 1:4){
rslt[i] <- 2*input_vals[i]
}
rslt
x <- asd %>% filter(group == 'ASD') %>% pull(CHIP)
y <- asd %>% filter(group == 'TD') %>% pull(CHIP)
t.test(x, y, var.equal = F)
x <- asd %>% filter(group == 'ASD') %>% pull(CHIP)
y <- asd %>% filter(group == 'TD') %>% pull(CHIP)
t.test(x, y, var.equal = F)
t.test(x, y) %>% str()
x <- asd %>% filter(group == 'ASD') %>% pull(CHIP)
y <- asd %>% filter(group == 'TD') %>% pull(CHIP)
t.test(x, y, var.equal = F)
t.test(x, y) %>% str()
t.test(x, y, var.equal = F)$p.value
# p-value is very important
n_tests <- 100
p_vals <- rep(NA, n_tests)
for(i in 1:n_tests){
x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)
y <- asd %>% filter(group == 'TD') %>% pull(i + 1)
p_vals[i] <- t.test(x, y, var.equal = F)$p.value
}
n_tests <- 100
p_vals <- rep(NA, n_tests)
for(i in 1:n_tests){
x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)
y <- asd %>% filter(group == 'TD') %>% pull(i + 1)
p_vals[i] <- t.test(x, y, var.equal = F)$p.value
}
tibble(protein = colnames(asd)[2:(n_tests + 1)],
p = p_vals)
n_tests <- 100
rslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],
p = NA)
for(i in 1:n_tests){
x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)
y <- asd %>% filter(group == 'TD') %>% pull(i + 1)
rslt$p[i] <- t.test(x, y, var.equal = F)$p.value
}
vals <- rnorm(n = 4)
simple_fn <- function(x){2*x}
lapply(vals, simple_fn)
vals <- rnorm(n = 4)
simple_fn <- function(x){2*x}
lapply(vals, simple_fn)
sapply(vals, simple_fn)
# apply a function to an index set
simple_fn_ix <- function(i){2*vals[i]}
rslt_apply <- sapply(1:length(vals), simple_fn_ix)
# equivalent for loop
rslt_loop <- rep(NA, length(vals))
for(i in 1:length(vals)){
rslt_loop[i] <- 2*vals[i]
}
# compare
rbind(rslt_loop, rslt_apply)
# number of tests to perform
n_tests <- 100
# convert to a list
asd_list <- asd %>%
select(1:(n_tests + 1)) %>%
pivot_longer(cols = -group,
names_to = 'protein',
values_to = 'level') %>%
group_by(protein) %>%
group_split()
# first entry in list
asd_list[[1]]
t.test(level ~ group, data = asd_list[[1]])
# p value for ith protein
tt_fn <- function(i){
t.test(level ~ group, data = asd_list[[i]])$p.value
}
# check
tt_fn(1)
sapply(1:n_tests, tt_fn)
start <- Sys.time()
rslt <- sapply(1:n_tests, tt_fn)
end <- Sys.time()
end - start
start <- Sys.time()
n_tests <- 100
rslt <- tibble(protein = colnames(asd)[2:(n_tests + 1)],
p = NA)
for(i in 1:n_tests){
x <- asd %>% filter(group == 'ASD') %>% pull(i + 1)
y <- asd %>% filter(group == 'TD') %>% pull(i + 1)
rslt$p[i] <- t.test(x, y, var.equal = F)$p.value
}
end <- Sys.time()
end - start
tt_fn <- function(i){
test_rslt <- t.test(level ~ group, data = asd_list[[i]])
out <- c(pval = test_rslt$p.value,
tstat = test_rslt$statistic)
out
}
tt_fn(1)
sapply(1:5, tt_fn) %>% t() %>% as_tibble()
# apply a function to an index set
simple_fn_ix <- function(i){2*vals[i]}
rslt_apply <- sapply(1:length(vals), simple_fn_ix)
# equivalent for loop
rslt_loop <- rep(NA, length(vals))
for(i in 1:length(vals)){
rslt_loop[i] <- 2*vals[i]
}
# compare
rbind(rslt_loop, rslt_apply)
asd_nested <- asd %>%
pivot_longer(-group,
names_to = 'protein',
values_to = 'level') %>%
nest(data = c(level, group))
asd_nested %>% head(5)
asd_nested %>%
slice(1L) %>%
pull(data)
asd_nested %>%
slice(1L) %>%
pull(data)
tt_fn <- function(.df){
t.test(level ~ group, data = .df)
}
rslt <- asd_nested %>%
slice(1:10) %>%
mutate(ttest.out = map(data, tt_fn))
rslt
rslt %>% slice(1L) %>% pull(ttest.out)
asd_nested %>%
slice(1L) %>%
unnest(cols = data) %>%
infer::t_test(formula = level ~ group,
order = c('ASD', 'TD'),
alternative = 'two-sided',
var.equal = F)
rslt %>% slice(1L) %>% pull(ttest.out)
# wrapper around infer::t_test
tt_fn <- function(.df){
infer::t_test(.df,
formula = level ~ group,
order = c('ASD', 'TD'),
alternative = 'two-sided',
var.equal = F)
}
# compute test results
tt_out <- asd_nested %>%
slice(1:n_tests) %>%
mutate(ttest = map(data, tt_fn))
# preview
tt_out %>% head(4)
tt_out %>%
unnest(ttest) %>%
head(4)
# time it
start <- Sys.time()
tt_out <- asd_nested %>%
slice(1:n_tests) %>%
mutate(ttest = map(data, tt_fn))
end <- Sys.time()
end - start
# bonferroni correction
tt_out %>%
unnest(ttest) %>%
mutate(p_adj = p_value*n_tests) %>%
select(protein, p_value, p_adj) %>%
arrange(p_adj) %>%
head(4)
library(tidyverse)
# data location
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
# import
census <- read_csv(url,
col_names = c("age",
"workclass",
"fnlwgt",
"education",
"education_1",
"marital_status",
"occupation",
"relationship",
"race",
"sex",
"capital_gain",
"capital_loss",
"hours_per_week",
"native_country",
"income")) %>%
mutate(income = factor(income)) %>%
select(-fnlwgt, -education_1)
census %>% head(4)
# inspect repsonse
census %>% pull(income) %>% str()
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
# resample data
census_boot <- census %>%
sample_n(size = 200, replace = T)
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
# comment out -- don't overwrite your bootstrap sample!
census_boot <- census %>%
sample_n(size = 200, replace = T)
# for continuous variables
census_boot %>%
ggplot(aes(x = age, # replace with predictor name
y = income)) +
geom_jitter(height = 0.1) +
geom_vline(xintercept = 35) # adjust cutoff
census_boot %>%
ggplot(aes(x = age, # replace with predictor name
y = ..density..)) +
geom_density(aes(color = income, fill = income),
alpha = 0.5) +
geom_vline(xintercept = 35) # adjust cutoff
# for categorical variables
census_boot %>%
group_by(workclass, income) %>%
count() %>%
spread(income, n) %>%
mutate_all(~ replace_na(.x, 0)) %>%
mutate(high.inc = `<=50K` > `>50K`)
# pick out categories that are majority high income
highinc_categories <- census_boot %>%
group_by(workclass, # replace with predictor name
income) %>%
count() %>%
spread(income, n) %>%
mutate_all(~ replace_na(.x, 0)) %>%
mutate(high.inc = `<=50K` > `>50K`) %>%
filter(high.inc == T) %>%
pull(workclass) # replace with predictor name
library(tidyverse)
# data location
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
# import
census <- read_csv(url,
col_names = c("age",
"workclass",
"fnlwgt",
"education",
"education_1",
"marital_status",
"occupation",
"relationship",
"race",
"sex",
"capital_gain",
"capital_loss",
"hours_per_week",
"native_country",
"income")) %>%
mutate(income = factor(income)) %>%
select(-fnlwgt, -education_1)
census %>% head(4)
# inspect repsonse
census %>% pull(income) %>% str()
# resample data
census_boot <- census %>%
sample_n(size = 200, replace = T)
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
# retrieve column names
possible_predictors <- census %>%
select(-income) %>%
colnames()
# grab 2 columns at random
predictors <- sample(possible_predictors,
size = 2,
replace = F)
# select these columns from the bootstrap sample
train <- census_boot %>%
select(c(income, any_of(predictors)))
train
# comment out -- don't overwrite your bootstrap sample!
census_boot <- census %>%
sample_n(size = 200, replace = T)
# for continuous variables
census_boot %>%
ggplot(aes(x = occupation, # replace with predictor name
y = income)) +
geom_jitter(height = 0.1) +
geom_vline(xintercept = 35) # adjust cutoff
# comment out -- don't overwrite your bootstrap sample!
census_boot <- census %>%
sample_n(size = 200, replace = T)
# for continuous variables
census_boot %>%
ggplot(aes(x = hours_per_week, # replace with predictor name
y = income)) +
geom_jitter(height = 0.1) +
geom_vline(xintercept = 35) # adjust cutoff
census_boot %>%
ggplot(aes(x = hours_per_week, # replace with predictor name
y = ..density..)) +
geom_density(aes(color = income, fill = income),
alpha = 0.5) +
geom_vline(xintercept = 35) # adjust cutoff
# pick out categories that are majority high income
highinc_categories <- census_boot %>%
group_by(workclass, # replace with predictor name
income) %>%
count() %>%
spread(income, n) %>%
mutate_all(~ replace_na(.x, 0)) %>%
mutate(high.inc = `<=50K` > `>50K`) %>%
filter(high.inc == T) %>%
pull(workclass) # replace with predictor name
# for categorical variables
census_boot %>%
group_by(education, income) %>%
count() %>%
spread(income, n) %>%
mutate_all(~ replace_na(.x, 0)) %>%
mutate(high.inc = `<=50K` > `>50K`)
# pick out categories that are majority high income
highinc_categories <- census_boot %>%
group_by(education, # replace with predictor name
income) %>%
count() %>%
spread(income, n) %>%
mutate_all(~ replace_na(.x, 0)) %>%
mutate(high.inc = `<=50K` > `>50K`) %>%
filter(high.inc == T) %>%
pull(education) # replace with predictor name
# comment out -- don't overwrite your bootstrap sample!
census_boot <- census %>%
sample_n(size = 200, replace = T)
# for continuous variables
census_boot %>%
ggplot(aes(x = age, # replace with predictor name
y = income)) +
geom_jitter(height = 0.1) +
geom_vline(xintercept = 35) # adjust cutoff
